{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/armin/anaconda/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We work with the reduced training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('reduced_training_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The method to create predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5 µs, sys: 1 µs, total: 6 µs\n",
      "Wall time: 9.06 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def create_top_5_predictions(user_indices, \n",
    "                             product_categories, \n",
    "                             X_train, \n",
    "                             y_train, \n",
    "                             X_test, \n",
    "                             previously_seen_items,\n",
    "                             max_depth_regression_tree = 3,\n",
    "                             verbose = False,\n",
    "                             sparse_computation = False,\n",
    "                            ):\n",
    "    \"\"\"Automatically determine the predictions for the predictions\n",
    "    \n",
    "    Keyword arguments:\n",
    "    user_ids -- A list of unique integers to guarantee consistency between different\n",
    "    product_categories -- A list of the id3s for which predictions should be made\n",
    "    X_train -- Feature vector, contains for each user_id in user_ids a vector of length len(product_categories)\n",
    "    y_train -- Feature vector, needs to have the same dimensions as X_train\n",
    "    X_test -- Feature vector, needs to have the same number of columns as X_train\n",
    "    previously_seen_items -- Same shape as X_train, lists all entries that have been seen in the three weeks before the \n",
    "    prediction was made\n",
    "    \"\"\"\n",
    "    prediction_frame = pd.DataFrame(index = user_indices, \n",
    "                                    columns = product_categories, \n",
    "                                    data = np.zeros((len(user_indices), len(product_categories))))\n",
    "\n",
    "    if sparse_computation:\n",
    "        # This option seems to be much faster but we need enough\n",
    "        # data for internal consistency\n",
    "        X_train_sparse = sparse.csr_matrix(X_train.values)\n",
    "        X_test_sparse = sparse.csr_matrix(X_test.values)\n",
    "    else:\n",
    "        X_train_sparse = X_train.values\n",
    "        X_test_sparse = X_test.values\n",
    "       \n",
    "    for id_to_predict in product_categories:\n",
    "        if verbose and id_to_predict % 50 == 0:\n",
    "            print(\"Currently predicting: %d \" % id_to_predict)\n",
    "\n",
    "        ### If you want to build a different predictor, change this here\n",
    "        ### Maybe build a decision tree classifier      \n",
    "        model = xgb.XGBRegressor(max_depth= max_depth_regression_tree)\n",
    "        model.fit(X_train_sparse, y_train[id_to_predict])\n",
    "        prediction_frame[id_to_predict] = model.predict(X_test_sparse)\n",
    "        \n",
    "        \n",
    "\n",
    "    # Find all the values that are eligible\n",
    "    # Fast way to compute which values can be used\n",
    "    unseen_items = np.ones(previously_seen_items.shape) - (previously_seen_items.values > 0)*1\n",
    "    valid_prediction = pd.DataFrame(data = np.multiply(unseen_items, prediction_frame.values),\n",
    "                                    index = user_indices, \n",
    "                                    columns = product_categories)\n",
    "    \n",
    "    @np.vectorize\n",
    "    def index_to_id3(x):\n",
    "        return product_categories[x]\n",
    "\n",
    "    # Compute the top 5 predictions\n",
    "    top_5_categories = np.argpartition(valid_prediction.values, -5)[:, -5:]\n",
    "    score = np.array([valid_prediction.values[i, top_5_categories[i,:]] for i in range(valid_prediction.shape[0])]).sum(axis=1)\n",
    "    ### SUPERIMPOTANT: Need to convert array indices of products into product categories!!!\n",
    "    result = pd.concat([pd.DataFrame(data = top_5_categories).apply(index_to_id3), pd.DataFrame(data = score)], axis=1)\n",
    "    result.index = valid_prediction.index\n",
    "    result.reset_index(inplace=True)\n",
    "    result.columns = ['user_id', 'id3_1', 'id3_2', 'id3_3', 'id3_4', 'id3_5', 'score']\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For the creation of the feature matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This method creates a feature matrix with user ids as rows and product_ids as columns\n",
    "# Each entry counts the number of different days the user (row index) as looked at id3 (column index)\n",
    "# This method can be used to create training data. As some user_ids or id3 may not be present in different\n",
    "# timeframes, indices or columns can be given in order to achieve consistency\n",
    "def create_feature_matrix(df_train, \n",
    "                          day_begin, \n",
    "                          day_end, \n",
    "                          indices=None, \n",
    "                          columns=None,\n",
    "                          column_name = 'id3'):\n",
    "    df_selected_entries = df_train[ (df_train['date'] >= day_begin)\n",
    "                                  & (df_train['date'] <= day_end)]\n",
    "    feature_matrix = pd.pivot_table(df_selected_entries, values='date',\n",
    "                                    columns=column_name, index='user_id',\n",
    "                                    aggfunc = pd.Series.nunique)\n",
    "    # We may need to add some additional indices if some users are not present during the time period\n",
    "    if not indices is None:\n",
    "        set_difference_indices = np.setdiff1d(indices, feature_matrix.index)\n",
    "        if len(set_difference_indices) > 0:\n",
    "            feature_matrix = pd.concat([feature_matrix, pd.DataFrame(index = set_difference_indices,\n",
    "                                                                    columns = feature_matrix.columns)])\n",
    "\n",
    "    # We may need to add some additional columns if some products are not present during the time period\n",
    "    if not columns is None:\n",
    "        set_difference_columns = np.setdiff1d(columns, feature_matrix.columns)\n",
    "        if len(set_difference_columns) > 0:\n",
    "            feature_matrix = pd.concat([feature_matrix, pd.DataFrame(index = feature_matrix.index,\n",
    "                                                                     columns = set_difference_columns)],\n",
    "                                       axis=1)\n",
    "    \n",
    "    feature_matrix.fillna(0, inplace=True)\n",
    "    # Sort the index\n",
    "    feature_matrix.sort_index(axis = 0, inplace=True)\n",
    "    # Sort the columns\n",
    "    feature_matrix.sort_index(axis = 1, inplace=True)\n",
    "       \n",
    "    return feature_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_extended_feature_matrix(df_train, \n",
    "                                   day_begin, \n",
    "                                   day_end, \n",
    "                                   indices, \n",
    "                                   columns_id1,\n",
    "                                   columns_id2,\n",
    "                                   columns_id3):\n",
    "    \"\"\" This concatenates features for all three columns\n",
    "    \n",
    "    \"\"\"\n",
    "    fm_id1 = create_feature_matrix(df_train, day_begin, day_end, indices, columns_id1, 'id1')\n",
    "    fm_id2 = create_feature_matrix(df_train, day_begin, day_end, indices, columns_id2, 'id2')\n",
    "    fm_id3 = create_feature_matrix(df_train, day_begin, day_end, indices, columns_id3, 'id3')\n",
    "    \n",
    "    return pd.concat([fm_id1, fm_id2, fm_id3], axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_sampled_training_dataset(df_train,\n",
    "                                    split_days, \n",
    "                                    train_period_length,\n",
    "                                    target_period_length,\n",
    "                                    indices, \n",
    "                                    columns_id1,\n",
    "                                    columns_id2,\n",
    "                                    columns_id3,\n",
    "                                    sample_fraction=0.1):\n",
    "    \n",
    "    first_iteration = True\n",
    "    for day in split_days:\n",
    "        X_tmp = create_feature_matrix(df_train = df_train,\n",
    "                                      day_begin = day - train_period_length,\n",
    "                                      day_end = day - 1,\n",
    "                                      indices,\n",
    "                                      columns = columns)\n",
    "        y_tmp = create_feature_matrix(df_train = df_train,\n",
    "                                      day_begin = day,\n",
    "                                      day_end = day + target_period_length - 1,\n",
    "                                      indices = indices,\n",
    "                                      columns = columns)\n",
    "\n",
    "        if first_iteration:\n",
    "            X_sampled = X_tmp\n",
    "            y_sampled = y_tmp\n",
    "            first_iteration = False\n",
    "        else:\n",
    "            X_sampled = pd.concat([X_sampled, X_tmp])\n",
    "            y_sampled = pd.concat([y_sampled, y_tmp])\n",
    "    \n",
    "    # Create the sampling\n",
    "    X_sampled['temp_index'] = np.array(range(X_sampled.shape[0]))\n",
    "    y_sampled['temp_index'] = np.array(range(X_sampled.shape[0]))\n",
    "    \n",
    "    X_sampled = X_sampled.sample(frac=sample_fraction)\n",
    "    X_sampled.sort_values(by='temp_index', inplace=True)\n",
    "    y_sampled = y_sampled[y_sampled['temp_index'].isin(X_sampled['temp_index'])]\n",
    "    y_sampled.sort_values(by='temp_index', inplace=True)\n",
    "    \n",
    "    X_sampled.drop('temp_index', axis=1, inplace=True)\n",
    "    y_sampled.drop('temp_index', axis=1, inplace=True)\n",
    "    \n",
    "    return X_sampled, y_sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now preparing the training\n",
      "Now computing\n",
      "Currently predicting: 0 \n",
      "Currently predicting: 50 \n",
      "Currently predicting: 100 \n",
      "Currently predicting: 150 \n",
      "Currently predicting: 200 \n",
      "Currently predicting: 250 \n",
      "Currently predicting: 300 \n",
      "Currently predicting: 350 \n",
      "Currently predicting: 400 \n",
      "Currently predicting: 450 \n",
      "Currently predicting: 500 \n",
      "Currently predicting: 550 \n",
      "Currently predicting: 600 \n",
      "Currently predicting: 650 \n",
      "Currently predicting: 700 \n",
      "Currently predicting: 750 \n",
      "Currently predicting: 800 \n",
      "Currently predicting: 850 \n",
      "Currently predicting: 900 \n",
      "Now printing\n",
      "CPU times: user 5h 29min 31s, sys: 5min 49s, total: 5h 35min 21s\n",
      "Wall time: 5h 36min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "day_split_test = 62\n",
    "day_split_train = 55\n",
    "\n",
    "# Sample the users to make everything a bit faster\n",
    "sampled_user = pd.Series(df_train['user_id'].unique()).sample(frac=1)\n",
    "df_user_sampled = df_train[df_train['user_id'].isin(sampled_user)]\n",
    "\n",
    "\n",
    "columns_id1 = sorted(df_train['id1'].unique())\n",
    "columns_id2 = sorted(df_train['id2'].unique())\n",
    "columns_id3 = sorted(df_train['id3'].unique())\n",
    "user_indices = sorted(df_user_sampled['user_id'].unique())\n",
    "user_indices_all = sorted(df_train['user_id'].unique())\n",
    "\n",
    "\n",
    "prediction_score = []\n",
    "previously_seen_items = create_feature_matrix(df_train = df_train, \n",
    "                                              day_begin = day_split_test - 21, \n",
    "                                              day_end = day_split_test - 1,\n",
    "                                              indices = user_indices_all, \n",
    "                                              columns = columns_id3,\n",
    "                                              column_name = 'id3'\n",
    "                                             )\n",
    "\n",
    "prediction_period = 9\n",
    "max_depth_regression_tree = 6\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Now preparing the training\")\n",
    "X_train = create_extended_feature_matrix(df_train = df_user_sampled, \n",
    "                                        day_begin = day_split_train - prediction_period, \n",
    "                                        day_end = day_split_train - 1,\n",
    "                                        indices = user_indices, \n",
    "                                        columns_id1 = columns_id1,\n",
    "                                        columns_id2 = columns_id2,\n",
    "                                        columns_id3 = columns_id3)   \n",
    "\n",
    "# IMPORTANT, we only want to predict the third category\n",
    "y_train = create_feature_matrix(df_train = df_user_sampled,\n",
    "                               day_begin = day_split_train,\n",
    "                               day_end = day_split_train + 6,\n",
    "                               indices = user_indices, \n",
    "                               columns = columns_id3,\n",
    "                               column_name = 'id3')\n",
    "X_test = create_extended_feature_matrix(df_train = df_user_sampled, \n",
    "                                       day_begin = day_split_test - prediction_period, \n",
    "                                       day_end = day_split_test - 1,\n",
    "                                       indices = user_indices, \n",
    "                                       columns_id1 = columns_id1,\n",
    "                                       columns_id2 = columns_id2,\n",
    "                                       columns_id3 = columns_id3)\n",
    "print(\"Now computing\")\n",
    "result = create_top_5_predictions(user_indices_all,\n",
    "                                  columns_id3,\n",
    "                                  X_train,\n",
    "                                  y_train,\n",
    "                                  X_test,\n",
    "                                  previously_seen_items,\n",
    "                                  max_depth_regression_tree = max_depth_regression_tree,\n",
    "                                  verbose = True,\n",
    "                                  sparse_computation = True,\n",
    "                                 )\n",
    "\n",
    "print(\"Now printing\")\n",
    "to_submit = result.nlargest(n=53979, columns='score')\n",
    "to_submit.to_csv('predictions/Prediction_AK_final_1.csv', \n",
    "                 columns=['user_id', 'id3_1', 'id3_2', 'id3_3', 'id3_4', 'id3_5'], \n",
    "                 index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
